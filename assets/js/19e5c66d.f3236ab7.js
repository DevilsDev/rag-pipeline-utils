"use strict";(self.webpackChunk_devilsdev_rag_pipeline_utils_docs=self.webpackChunk_devilsdev_rag_pipeline_utils_docs||[]).push([[2374],{8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>l});var s=r(6540);const i={},t=s.createContext(i);function a(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(t.Provider,{value:n},e.children)}},9892:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Evaluation","title":"Evaluation Framework","description":"The @DevilsDev/rag-pipeline-utils evaluation framework provides a comprehensive suite of metrics, methodologies, and tools for assessing RAG pipeline performance across multiple dimensions. This system enables rigorous testing, benchmarking, and continuous improvement of retrieval-augmented generation systems.","source":"@site/docs/Evaluation.md","sourceDirName":".","slug":"/Evaluation","permalink":"/rag-pipeline-utils/docs/Evaluation","draft":false,"unlisted":false,"editUrl":"https://github.com/DevilsDev/rag-pipeline-utils/edit/develop/docs-site/docs/Evaluation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Security Guide","permalink":"/rag-pipeline-utils/docs/Security"},"next":{"title":"Performance Optimization Guide","permalink":"/rag-pipeline-utils/docs/Performance"}}');var i=r(4848),t=r(8453);const a={},l="Evaluation Framework",o={},c=[{value:"\ud83d\udcca <strong>Core Evaluation Metrics</strong>",id:"-core-evaluation-metrics",level:2},{value:"<strong>1. BLEU Score (Bilingual Evaluation Understudy)</strong>",id:"1-bleu-score-bilingual-evaluation-understudy",level:3},{value:"<strong>2. ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)</strong>",id:"2-rouge-score-recall-oriented-understudy-for-gisting-evaluation",level:3},{value:"<strong>3. BERTScore</strong>",id:"3-bertscore",level:3},{value:"<strong>4. Semantic Similarity</strong>",id:"4-semantic-similarity",level:3},{value:"<strong>5. Advanced Metrics</strong>",id:"5-advanced-metrics",level:3},{value:"\ud83d\udd2c <strong>Evaluation Methodologies</strong>",id:"-evaluation-methodologies",level:2},{value:"<strong>1. Batch Evaluation</strong>",id:"1-batch-evaluation",level:3},{value:"<strong>2. A/B Testing Framework</strong>",id:"2-ab-testing-framework",level:3},{value:"<strong>3. Continuous Evaluation</strong>",id:"3-continuous-evaluation",level:3},{value:"\ud83d\udee0\ufe0f <strong>CLI Evaluation Tools</strong>",id:"\ufe0f-cli-evaluation-tools",level:2},{value:"<strong>Basic Evaluation</strong>",id:"basic-evaluation",level:3},{value:"<strong>Advanced Evaluation Options</strong>",id:"advanced-evaluation-options",level:3},{value:"<strong>Report Generation</strong>",id:"report-generation",level:3},{value:"\ud83d\udcc8 <strong>Performance Benchmarking</strong>",id:"-performance-benchmarking",level:2},{value:"<strong>Component Benchmarking</strong>",id:"component-benchmarking",level:3},{value:"<strong>End-to-End Performance</strong>",id:"end-to-end-performance",level:3},{value:"\ud83d\udcca <strong>Dataset Format &amp; Management</strong>",id:"-dataset-format--management",level:2},{value:"<strong>Standard Dataset Format</strong>",id:"standard-dataset-format",level:3},{value:"<strong>Dataset Validation</strong>",id:"dataset-validation",level:3},{value:"<strong>Dataset Generation</strong>",id:"dataset-generation",level:3},{value:"\ud83d\udcca <strong>Dashboard Visualization</strong>",id:"-dashboard-visualization",level:2},{value:"<strong>Interactive Dashboard</strong>",id:"interactive-dashboard",level:3},{value:"<strong>Custom Dashboard Integration</strong>",id:"custom-dashboard-integration",level:3},{value:"\ud83c\udfaf <strong>Best Practices</strong>",id:"-best-practices",level:2},{value:"<strong>1. Test Data Quality</strong>",id:"1-test-data-quality",level:3},{value:"<strong>2. Metric Selection</strong>",id:"2-metric-selection",level:3},{value:"<strong>3. Evaluation Frequency</strong>",id:"3-evaluation-frequency",level:3},{value:"<strong>4. Result Analysis</strong>",id:"4-result-analysis",level:3},{value:"\ud83d\udd27 <strong>Configuration Examples</strong>",id:"-configuration-examples",level:2},{value:"<strong>Comprehensive Evaluation Config</strong>",id:"comprehensive-evaluation-config",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"evaluation-framework",children:"Evaluation Framework"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"@DevilsDev/rag-pipeline-utils"})," evaluation framework provides a comprehensive suite of metrics, methodologies, and tools for assessing RAG pipeline performance across multiple dimensions. This system enables rigorous testing, benchmarking, and continuous improvement of retrieval-augmented generation systems."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-core-evaluation-metrics",children:["\ud83d\udcca ",(0,i.jsx)(n.strong,{children:"Core Evaluation Metrics"})]}),"\n",(0,i.jsx)(n.h3,{id:"1-bleu-score-bilingual-evaluation-understudy",children:(0,i.jsx)(n.strong,{children:"1. BLEU Score (Bilingual Evaluation Understudy)"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Measures n-gram overlap between generated and reference text, originally designed for machine translation evaluation."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Characteristics"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Range"}),": 0.0 to 1.0 (higher is better)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Strengths"}),": Fast computation, widely adopted standard"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Limitations"}),": Sensitive to exact word matches, may miss semantic equivalence"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implementation"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:'import { calculateBLEU } from \'@DevilsDev/rag-pipeline-utils\';\r\n\r\nconst bleuScore = calculateBLEU({\r\n  candidate: "The system uses vector embeddings for retrieval",\r\n  reference: "Vector embeddings are used by the system for retrieval",\r\n  nGrams: [1, 2, 3, 4], // BLEU-1 through BLEU-4\r\n  smoothing: true\r\n});\r\n\r\nconsole.log(`BLEU Score: ${bleuScore.overall}`);\r\nconsole.log(`BLEU-1: ${bleuScore.bleu1}`);\r\nconsole.log(`BLEU-4: ${bleuScore.bleu4}`);\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-rouge-score-recall-oriented-understudy-for-gisting-evaluation",children:(0,i.jsx)(n.strong,{children:"2. ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Evaluates recall-oriented text summarization and generation quality."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Variants"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROUGE-1"}),": Unigram overlap (word-level recall)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROUGE-2"}),": Bigram overlap (phrase-level recall)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROUGE-L"}),": Longest Common Subsequence (structural similarity)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROUGE-W"}),": Weighted Longest Common Subsequence"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implementation"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { calculateROUGE } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst rougeScores = calculateROUGE({\r\n  candidate: \"RAG systems combine retrieval with generation for better accuracy\",\r\n  reference: \"Retrieval-augmented generation improves accuracy by combining retrieval and generation\",\r\n  variants: ['rouge-1', 'rouge-2', 'rouge-l'],\r\n  stemming: true,\r\n  stopwordRemoval: true\r\n});\r\n\r\nconsole.log('ROUGE Scores:', {\r\n  'ROUGE-1': rougeScores.rouge1,\r\n  'ROUGE-2': rougeScores.rouge2,\r\n  'ROUGE-L': rougeScores.rougeL\r\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-bertscore",children:(0,i.jsx)(n.strong,{children:"3. BERTScore"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Uses contextual embeddings (BERT-based) for semantic similarity evaluation, more robust to paraphrasing than n-gram metrics."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Captures semantic meaning beyond surface-level text"}),"\n",(0,i.jsx)(n.li,{children:"Handles synonyms and paraphrasing effectively"}),"\n",(0,i.jsx)(n.li,{children:"Correlates better with human judgment"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implementation"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { calculateBERTScore } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst bertScore = await calculateBERTScore({\r\n  candidate: \"The model leverages transformer architecture\",\r\n  reference: \"Transformer-based architecture is used by the model\",\r\n  model: 'bert-base-uncased', // or 'roberta-base', 'distilbert-base'\r\n  language: 'en',\r\n  rescaleWithBaseline: true\r\n});\r\n\r\nconsole.log('BERTScore:', {\r\n  precision: bertScore.precision,\r\n  recall: bertScore.recall,\r\n  f1: bertScore.f1\r\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-semantic-similarity",children:(0,i.jsx)(n.strong,{children:"4. Semantic Similarity"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Measures cosine similarity between sentence embeddings to capture semantic meaning."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implementation"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { calculateSemanticSimilarity } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst similarity = await calculateSemanticSimilarity({\r\n  text1: \"Vector databases store high-dimensional embeddings\",\r\n  text2: \"High-dimensional embeddings are stored in vector databases\",\r\n  model: 'sentence-transformers/all-MiniLM-L6-v2',\r\n  normalize: true\r\n});\r\n\r\nconsole.log(`Semantic Similarity: ${similarity.score}`);\r\nconsole.log(`Confidence: ${similarity.confidence}`);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"5-advanced-metrics",children:(0,i.jsx)(n.strong,{children:"5. Advanced Metrics"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Faithfulness"}),": Measures how well the generated answer is supported by the retrieved context."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:'import { calculateFaithfulness } from \'@DevilsDev/rag-pipeline-utils\';\r\n\r\nconst faithfulness = await calculateFaithfulness({\r\n  answer: "The system uses OpenAI embeddings for vector search",\r\n  context: [\r\n    "The pipeline integrates with OpenAI\'s embedding API",\r\n    "Vector search is performed using cosine similarity"\r\n  ],\r\n  model: \'openai-gpt-4\'\r\n});\r\n\r\nconsole.log(`Faithfulness Score: ${faithfulness.score}`);\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Answer Relevance"}),": Evaluates how well the answer addresses the original question."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { calculateAnswerRelevance } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst relevance = await calculateAnswerRelevance({\r\n  question: \"How does the retrieval system work?\",\r\n  answer: \"The retrieval system uses vector embeddings to find similar documents\",\r\n  model: 'openai-gpt-4'\r\n});\r\n\r\nconsole.log(`Answer Relevance: ${relevance.score}`);\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Context Precision"}),": Measures the precision of retrieved context chunks."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:'import { calculateContextPrecision } from \'@DevilsDev/rag-pipeline-utils\';\r\n\r\nconst precision = await calculateContextPrecision({\r\n  question: "What is RAG?",\r\n  contexts: [\r\n    "RAG combines retrieval and generation",\r\n    "Weather forecast for tomorrow", // irrelevant\r\n    "Retrieval-augmented generation improves accuracy"\r\n  ],\r\n  groundTruthAnswer: "RAG is retrieval-augmented generation"\r\n});\r\n\r\nconsole.log(`Context Precision: ${precision.score}`);\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-evaluation-methodologies",children:["\ud83d\udd2c ",(0,i.jsx)(n.strong,{children:"Evaluation Methodologies"})]}),"\n",(0,i.jsx)(n.h3,{id:"1-batch-evaluation",children:(0,i.jsx)(n.strong,{children:"1. Batch Evaluation"})}),"\n",(0,i.jsx)(n.p,{children:"Evaluate multiple queries systematically:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { evaluateResults, loadTestData } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst testData = await loadTestData('./evaluation-dataset.json');\r\n\r\nconst results = await evaluateResults({\r\n  queries: testData.queries,\r\n  expectedAnswers: testData.groundTruth,\r\n  pipeline: myRagPipeline,\r\n  metrics: {\r\n    bleu: { nGrams: [1, 2, 3, 4] },\r\n    rouge: { variants: ['rouge-1', 'rouge-2', 'rouge-l'] },\r\n    bertscore: { model: 'bert-base-uncased' },\r\n    semantic: { model: 'sentence-transformers/all-MiniLM-L6-v2' },\r\n    faithfulness: { model: 'openai-gpt-4' },\r\n    answerRelevance: { model: 'openai-gpt-4' },\r\n    contextPrecision: { threshold: 0.7 }\r\n  },\r\n  batchSize: 10,\r\n  parallel: 3\r\n});\r\n\r\nconsole.log('Evaluation Results:', {\r\n  averageScores: results.averageScores,\r\n  totalQueries: results.totalQueries,\r\n  processingTime: results.processingTime,\r\n  failedQueries: results.failedQueries.length\r\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-ab-testing-framework",children:(0,i.jsx)(n.strong,{children:"2. A/B Testing Framework"})}),"\n",(0,i.jsx)(n.p,{children:"Compare different pipeline configurations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { compareConfigurations } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst comparison = await compareConfigurations({\r\n  configurations: {\r\n    'baseline': {\r\n      embedder: 'openai',\r\n      llm: 'openai-gpt-3.5-turbo',\r\n      chunkSize: 1000\r\n    },\r\n    'optimized': {\r\n      embedder: 'openai',\r\n      llm: 'openai-gpt-4',\r\n      chunkSize: 1500,\r\n      reranker: 'cohere'\r\n    }\r\n  },\r\n  testQueries: testData.queries,\r\n  groundTruth: testData.expectedAnswers,\r\n  metrics: ['bleu', 'rouge', 'bertscore', 'faithfulness'],\r\n  statisticalSignificance: true\r\n});\r\n\r\nconsole.log('A/B Test Results:', comparison.summary);\r\nconsole.log('Statistical Significance:', comparison.significance);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-continuous-evaluation",children:(0,i.jsx)(n.strong,{children:"3. Continuous Evaluation"})}),"\n",(0,i.jsx)(n.p,{children:"Set up automated evaluation pipelines:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { createEvaluationPipeline } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst evaluationPipeline = createEvaluationPipeline({\r\n  schedule: '0 2 * * *', // Daily at 2 AM\r\n  testSuite: './test-suites/regression.json',\r\n  baselineThresholds: {\r\n    bleu: 0.3,\r\n    rouge: 0.4,\r\n    bertscore: 0.8,\r\n    faithfulness: 0.7\r\n  },\r\n  notifications: {\r\n    slack: process.env.SLACK_WEBHOOK,\r\n    email: ['team@company.com']\r\n  },\r\n  reportGeneration: {\r\n    format: ['json', 'html', 'pdf'],\r\n    outputDir: './evaluation-reports'\r\n  }\r\n});\r\n\r\nevaluationPipeline.start();\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"\ufe0f-cli-evaluation-tools",children:["\ud83d\udee0\ufe0f ",(0,i.jsx)(n.strong,{children:"CLI Evaluation Tools"})]}),"\n",(0,i.jsx)(n.h3,{id:"basic-evaluation",children:(0,i.jsx)(n.strong,{children:"Basic Evaluation"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Evaluate with default metrics\r\nrag-pipeline evaluate ./test-queries.json --output results.csv\r\n\r\n# Specify custom metrics\r\nrag-pipeline evaluate ./queries.json \\\r\n  --metrics bleu,rouge,bertscore,semantic \\\r\n  --output detailed-results.json\r\n\r\n# Evaluation with ground truth\r\nrag-pipeline evaluate ./queries.json \\\r\n  --ground-truth ./expected-answers.json \\\r\n  --metrics all \\\r\n  --detailed\n"})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-evaluation-options",children:(0,i.jsx)(n.strong,{children:"Advanced Evaluation Options"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Parallel evaluation with custom batch size\r\nrag-pipeline evaluate ./large-test-set.json \\\r\n  --batch-size 20 \\\r\n  --parallel 4 \\\r\n  --timeout 30\r\n\r\n# Comparative evaluation\r\nrag-pipeline evaluate ./queries.json \\\r\n  --compare-configs baseline.json,optimized.json \\\r\n  --output comparison-report.html\r\n\r\n# Evaluation with custom models\r\nrag-pipeline evaluate ./queries.json \\\r\n  --bertscore-model roberta-large \\\r\n  --semantic-model sentence-transformers/all-mpnet-base-v2\n"})}),"\n",(0,i.jsx)(n.h3,{id:"report-generation",children:(0,i.jsx)(n.strong,{children:"Report Generation"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Generate comprehensive HTML report\r\nrag-pipeline evaluate ./queries.json \\\r\n  --output-format html \\\r\n  --include-charts \\\r\n  --include-examples\r\n\r\n# Export to multiple formats\r\nrag-pipeline evaluate ./queries.json \\\r\n  --output results \\\r\n  --formats json,csv,xlsx,html\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-performance-benchmarking",children:["\ud83d\udcc8 ",(0,i.jsx)(n.strong,{children:"Performance Benchmarking"})]}),"\n",(0,i.jsx)(n.h3,{id:"component-benchmarking",children:(0,i.jsx)(n.strong,{children:"Component Benchmarking"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { benchmarkComponents } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst benchmarks = await benchmarkComponents({\r\n  components: ['embedder', 'retriever', 'llm'],\r\n  testData: {\r\n    documents: './benchmark-docs/',\r\n    queries: './benchmark-queries.json'\r\n  },\r\n  metrics: {\r\n    latency: true,\r\n    throughput: true,\r\n    accuracy: true,\r\n    resourceUsage: true\r\n  },\r\n  loadPatterns: {\r\n    concurrent: [1, 5, 10, 20],\r\n    duration: 300 // seconds\r\n  }\r\n});\r\n\r\nconsole.log('Benchmark Results:', benchmarks.summary);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"end-to-end-performance",children:(0,i.jsx)(n.strong,{children:"End-to-End Performance"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Full pipeline benchmark\r\nrag-pipeline benchmark \\\r\n  --queries 1000 \\\r\n  --concurrent 10 \\\r\n  --duration 600 \\\r\n  --output benchmark-report.json\r\n\r\n# Stress testing\r\nrag-pipeline benchmark --stress \\\r\n  --max-concurrent 50 \\\r\n  --ramp-up 120 \\\r\n  --target-rps 100\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-dataset-format--management",children:["\ud83d\udcca ",(0,i.jsx)(n.strong,{children:"Dataset Format & Management"})]}),"\n",(0,i.jsx)(n.h3,{id:"standard-dataset-format",children:(0,i.jsx)(n.strong,{children:"Standard Dataset Format"})}),"\n",(0,i.jsx)(n.p,{children:"Each entry in your JSON dataset should include:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\r\n  "id": "query-001",\r\n  "prompt": "What is a vector index?",\r\n  "groundTruth": "A data structure for fast similarity search in vector spaces.",\r\n  "category": "technical",\r\n  "difficulty": "intermediate",\r\n  "metadata": {\r\n    "domain": "machine-learning",\r\n    "expectedLength": "short",\r\n    "requiresContext": true\r\n  }\r\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"dataset-validation",children:(0,i.jsx)(n.strong,{children:"Dataset Validation"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { validateDataset } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst validation = await validateDataset('./evaluation-dataset.json', {\r\n  requiredFields: ['prompt', 'groundTruth'],\r\n  optionalFields: ['category', 'difficulty', 'metadata'],\r\n  minEntries: 10,\r\n  maxPromptLength: 500,\r\n  maxGroundTruthLength: 1000\r\n});\r\n\r\nif (!validation.isValid) {\r\n  console.error('Dataset validation failed:', validation.errors);\r\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"dataset-generation",children:(0,i.jsx)(n.strong,{children:"Dataset Generation"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { generateSyntheticDataset } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst syntheticData = await generateSyntheticDataset({\r\n  sourceDocuments: './knowledge-base/',\r\n  numQueries: 100,\r\n  questionTypes: ['factual', 'analytical', 'comparative'],\r\n  difficultyLevels: ['easy', 'medium', 'hard'],\r\n  llm: 'openai-gpt-4'\r\n});\r\n\r\nconsole.log(`Generated ${syntheticData.length} synthetic query-answer pairs`);\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-dashboard-visualization",children:["\ud83d\udcca ",(0,i.jsx)(n.strong,{children:"Dashboard Visualization"})]}),"\n",(0,i.jsx)(n.h3,{id:"interactive-dashboard",children:(0,i.jsx)(n.strong,{children:"Interactive Dashboard"})}),"\n",(0,i.jsx)(n.p,{children:"Use the React evaluation dashboard:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Start the evaluation dashboard\r\nrag-pipeline dashboard --port 3000 --data ./evaluation-results.json\r\n\r\n# Or use the built-in dashboard server\r\nnode public/server.js\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Visit: ",(0,i.jsx)(n.a,{href:"http://localhost:3000",children:"http://localhost:3000"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Dashboard Features"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Metrics"}),": Live updating evaluation scores"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Interactive Charts"}),": BLEU, ROUGE, BERTScore visualizations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Query Analysis"}),": Detailed breakdown of individual queries"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Comparison Views"}),": Side-by-side configuration comparisons"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Export Options"}),": CSV, JSON, PDF report generation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Filtering & Search"}),": Advanced result filtering capabilities"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"custom-dashboard-integration",children:(0,i.jsx)(n.strong,{children:"Custom Dashboard Integration"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { createDashboardAPI } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst dashboardAPI = createDashboardAPI({\r\n  dataSource: './evaluation-results.json',\r\n  refreshInterval: 30000, // 30 seconds\r\n  customMetrics: {\r\n    'domain_accuracy': (result) => calculateDomainAccuracy(result),\r\n    'response_completeness': (result) => calculateCompleteness(result)\r\n  }\r\n});\r\n\r\n// Serve dashboard data via REST API\r\napp.get('/api/evaluation-data', dashboardAPI.getEvaluationData);\r\napp.get('/api/metrics-summary', dashboardAPI.getMetricsSummary);\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-best-practices",children:["\ud83c\udfaf ",(0,i.jsx)(n.strong,{children:"Best Practices"})]}),"\n",(0,i.jsx)(n.h3,{id:"1-test-data-quality",children:(0,i.jsx)(n.strong,{children:"1. Test Data Quality"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Diverse Queries"}),": Include various question types, complexity levels, and domains"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High-Quality Ground Truth"}),": Ensure reference answers are accurate and comprehensive"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Balanced Dataset"}),": Represent different use cases and edge cases"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Regular Updates"}),": Keep test data current with evolving requirements"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-metric-selection",children:(0,i.jsx)(n.strong,{children:"2. Metric Selection"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multiple Metrics"}),": Use complementary metrics for comprehensive evaluation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Domain-Specific"}),": Include metrics relevant to your specific use case"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human Evaluation"}),": Supplement automated metrics with human judgment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Baseline Comparison"}),": Establish and maintain performance baselines"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-evaluation-frequency",children:(0,i.jsx)(n.strong,{children:"3. Evaluation Frequency"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Continuous Integration"}),": Run basic evaluations on every code change"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Nightly Builds"}),": Comprehensive evaluation suites during off-hours"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Release Gates"}),": Thorough evaluation before production deployments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Monitoring"}),": Real-time evaluation in production environments"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-result-analysis",children:(0,i.jsx)(n.strong,{children:"4. Result Analysis"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Statistical Significance"}),": Ensure results are statistically meaningful"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Analysis"}),": Investigate failure cases and edge conditions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Trend Monitoring"}),": Track performance changes over time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actionable Insights"}),": Convert evaluation results into improvement actions"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-configuration-examples",children:["\ud83d\udd27 ",(0,i.jsx)(n.strong,{children:"Configuration Examples"})]}),"\n",(0,i.jsx)(n.h3,{id:"comprehensive-evaluation-config",children:(0,i.jsx)(n.strong,{children:"Comprehensive Evaluation Config"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\r\n  "evaluation": {\r\n    "metrics": {\r\n      "bleu": {\r\n        "enabled": true,\r\n        "nGrams": [1, 2, 3, 4],\r\n        "smoothing": true,\r\n        "weight": 0.2\r\n      },\r\n      "rouge": {\r\n        "enabled": true,\r\n        "variants": ["rouge-1", "rouge-2", "rouge-l"],\r\n        "stemming": true,\r\n        "stopwordRemoval": true,\r\n        "weight": 0.2\r\n      },\r\n      "bertscore": {\r\n        "enabled": true,\r\n        "model": "bert-base-uncased",\r\n        "rescaleWithBaseline": true,\r\n        "weight": 0.3\r\n      },\r\n      "semantic": {\r\n        "enabled": true,\r\n        "model": "sentence-transformers/all-MiniLM-L6-v2",\r\n        "normalize": true,\r\n        "weight": 0.15\r\n      },\r\n      "faithfulness": {\r\n        "enabled": true,\r\n        "model": "openai-gpt-4",\r\n        "weight": 0.15\r\n      }\r\n    },\r\n    "thresholds": {\r\n      "bleu": 0.3,\r\n      "rouge": 0.4,\r\n      "bertscore": 0.8,\r\n      "semantic": 0.75,\r\n      "faithfulness": 0.7\r\n    },\r\n    "reporting": {\r\n      "formats": ["json", "html", "csv"],\r\n      "includeCharts": true,\r\n      "includeExamples": true,\r\n      "maxExamples": 10\r\n    }\r\n  }\r\n}\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["This comprehensive evaluation framework enables rigorous assessment of RAG pipeline performance across multiple dimensions. For implementation details, see the ",(0,i.jsx)(n.a,{href:"/rag-pipeline-utils/docs/Usage",children:"Usage Guide"}),", or explore ",(0,i.jsx)(n.a,{href:"/rag-pipeline-utils/docs/CLI",children:"CLI Reference"})," for command-line evaluation tools."]})})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);