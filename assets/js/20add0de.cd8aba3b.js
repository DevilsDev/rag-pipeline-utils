"use strict";(self.webpackChunk_devilsdev_rag_pipeline_utils_docs=self.webpackChunk_devilsdev_rag_pipeline_utils_docs||[]).push([[5939],{4943:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Performance","title":"Performance Optimization Guide","description":"This comprehensive performance guide helps you optimize @DevilsDev/rag-pipeline-utils for maximum throughput, minimal latency, and efficient resource utilization. From embedding generation to vector retrieval and LLM inference, this guide covers optimization strategies for every component.","source":"@site/docs/Performance.md","sourceDirName":".","slug":"/Performance","permalink":"/rag-pipeline-utils/docs/Performance","draft":false,"unlisted":false,"editUrl":"https://github.com/DevilsDev/rag-pipeline-utils/edit/develop/docs-site/docs/Performance.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Evaluation Framework","permalink":"/rag-pipeline-utils/docs/Evaluation"},"next":{"title":"Migration Guide","permalink":"/rag-pipeline-utils/docs/Migration"}}');var i=r(4848),s=r(8453);const o={},a="Performance Optimization Guide",c={},l=[{value:"\ud83d\ude80 <strong>Performance Overview</strong>",id:"-performance-overview",level:2},{value:"<strong>Key Performance Metrics</strong>",id:"key-performance-metrics",level:3},{value:"<strong>Performance Monitoring</strong>",id:"performance-monitoring",level:3},{value:"\ud83d\udcca <strong>Component-Specific Optimization</strong>",id:"-component-specific-optimization",level:2},{value:"<strong>1. Embedding Performance</strong>",id:"1-embedding-performance",level:3},{value:"<strong>2. Vector Retrieval Optimization</strong>",id:"2-vector-retrieval-optimization",level:3},{value:"<strong>3. LLM Generation Optimization</strong>",id:"3-llm-generation-optimization",level:3},{value:"\ud83d\udd04 <strong>Pipeline-Level Optimization</strong>",id:"-pipeline-level-optimization",level:2},{value:"<strong>Streaming Architecture</strong>",id:"streaming-architecture",level:3},{value:"<strong>Memory Management</strong>",id:"memory-management",level:3},{value:"<strong>Caching Strategy</strong>",id:"caching-strategy",level:3},{value:"\ud83d\udcc8 <strong>Monitoring &amp; Profiling</strong>",id:"-monitoring--profiling",level:2},{value:"<strong>Performance Metrics Collection</strong>",id:"performance-metrics-collection",level:3},{value:"<strong>Profiling Tools</strong>",id:"profiling-tools",level:3},{value:"<strong>Custom Metrics</strong>",id:"custom-metrics",level:3},{value:"\u26a1 <strong>Production Optimization</strong>",id:"-production-optimization",level:2},{value:"<strong>Deployment Configuration</strong>",id:"deployment-configuration",level:3},{value:"<strong>Load Balancing</strong>",id:"load-balancing",level:3},{value:"<strong>Auto-scaling Configuration</strong>",id:"auto-scaling-configuration",level:3},{value:"\ud83c\udfaf <strong>Performance Best Practices</strong>",id:"-performance-best-practices",level:2},{value:"<strong>1. Configuration Optimization</strong>",id:"1-configuration-optimization",level:3},{value:"<strong>2. Resource Management</strong>",id:"2-resource-management",level:3},{value:"<strong>3. Monitoring Checklist</strong>",id:"3-monitoring-checklist",level:3},{value:"\ud83d\udcca <strong>Performance Benchmarks</strong>",id:"-performance-benchmarks",level:2},{value:"<strong>Reference Performance</strong>",id:"reference-performance",level:3},{value:"<strong>Optimization Targets</strong>",id:"optimization-targets",level:3}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"performance-optimization-guide",children:"Performance Optimization Guide"})}),"\n",(0,i.jsxs)(n.p,{children:["This comprehensive performance guide helps you optimize ",(0,i.jsx)(n.strong,{children:"@DevilsDev/rag-pipeline-utils"})," for maximum throughput, minimal latency, and efficient resource utilization. From embedding generation to vector retrieval and LLM inference, this guide covers optimization strategies for every component."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-performance-overview",children:["\ud83d\ude80 ",(0,i.jsx)(n.strong,{children:"Performance Overview"})]}),"\n",(0,i.jsx)(n.h3,{id:"key-performance-metrics",children:(0,i.jsx)(n.strong,{children:"Key Performance Metrics"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Throughput"}),": Documents processed per second"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency"}),": Time from query to response"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory Usage"}),": RAM consumption during processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Token Efficiency"}),": Cost optimization for LLM usage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Concurrent Processing"}),": Parallel operation capabilities"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-monitoring",children:(0,i.jsx)(n.strong,{children:"Performance Monitoring"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Enable performance monitoring\r\nrag-pipeline query "test" --benchmark --stats\r\n\r\n# Comprehensive performance analysis\r\nrag-pipeline benchmark --comprehensive --output perf-report.json\r\n\r\n# Real-time monitoring\r\nrag-pipeline monitor --metrics throughput,latency,memory --interval 5s\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-component-specific-optimization",children:["\ud83d\udcca ",(0,i.jsx)(n.strong,{children:"Component-Specific Optimization"})]}),"\n",(0,i.jsx)(n.h3,{id:"1-embedding-performance",children:(0,i.jsx)(n.strong,{children:"1. Embedding Performance"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Batch Processing Optimization"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { createRagPipeline } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst pipeline = createRagPipeline({\r\n  embedder: {\r\n    name: 'openai',\r\n    config: {\r\n      batchSize: 100,           // Process 100 texts at once\r\n      maxConcurrency: 5,        // 5 parallel API calls\r\n      timeout: 30000,           // 30 second timeout\r\n      retryAttempts: 3,         // Retry failed requests\r\n      cacheEnabled: true,       // Cache embeddings\r\n      cacheSize: 10000         // Cache up to 10k embeddings\r\n    }\r\n  }\r\n});\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parallel Embedding Strategy"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Custom parallel embedding implementation\r\nclass OptimizedEmbedder extends BaseEmbedder {\r\n  constructor(options = {}) {\r\n    super();\r\n    this.concurrencyLimit = options.concurrency || 5;\r\n    this.batchSize = options.batchSize || 50;\r\n    this.cache = new LRUCache({ max: options.cacheSize || 5000 });\r\n  }\r\n\r\n  async embedBatch(texts, options = {}) {\r\n    // Filter cached texts\r\n    const uncachedTexts = texts.filter(text =&gt; !this.cache.has(text));\r\n    \r\n    if (uncachedTexts.length === 0) {\r\n      return texts.map(text =&gt; this.cache.get(text));\r\n    }\r\n\r\n    // Create batches for parallel processing\r\n    const batches = this.createBatches(uncachedTexts, this.batchSize);\r\n    \r\n    // Process batches with concurrency limit\r\n    const results = await this.processBatchesParallel(batches, {\r\n      concurrency: this.concurrencyLimit,\r\n      retryOnFailure: true\r\n    });\r\n\r\n    // Cache results\r\n    results.forEach((embedding, index) =&gt; {\r\n      this.cache.set(uncachedTexts[index], embedding);\r\n    });\r\n\r\n    return texts.map(text =&gt; this.cache.get(text));\r\n  }\r\n\r\n  async processBatchesParallel(batches, options) {\r\n    const semaphore = new Semaphore(options.concurrency);\r\n    \r\n    return Promise.all(\r\n      batches.map(async (batch) =&gt; {\r\n        await semaphore.acquire();\r\n        try {\r\n          return await this.processBatch(batch);\r\n        } finally {\r\n          semaphore.release();\r\n        }\r\n      })\r\n    );\r\n  }\r\n}\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Embedding Benchmarks"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Benchmark different embedding strategies\r\nrag-pipeline benchmark embedder \\\r\n  --texts 1000 \\\r\n  --batch-sizes 10,50,100,200 \\\r\n  --concurrency-levels 1,3,5,10 \\\r\n  --output embedding-benchmark.json\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-vector-retrieval-optimization",children:(0,i.jsx)(n.strong,{children:"2. Vector Retrieval Optimization"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Index Configuration"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"const pipeline = createRagPipeline({\r\n  retriever: {\r\n    name: 'pinecone',\r\n    config: {\r\n      indexType: 'approximateSearch',  // Faster but less precise\r\n      dimensions: 1536,\r\n      metric: 'cosine',\r\n      pods: 1,\r\n      replicas: 1,\r\n      podType: 'p1.x1',              // Optimize for your workload\r\n      \r\n      // Query optimization\r\n      topK: 5,                       // Limit results\r\n      includeMetadata: false,        // Reduce payload size\r\n      includeValues: false,          // Reduce payload size\r\n      \r\n      // Connection pooling\r\n      maxConnections: 10,\r\n      keepAlive: true,\r\n      timeout: 5000\r\n    }\r\n  }\r\n});\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Query Optimization"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Optimized retrieval with filtering\r\nasync function optimizedRetrieve(query, options = {}) {\r\n  const queryVector = await this.embedder.embed(query);\r\n  \r\n  return await this.retriever.retrieve(queryVector, {\r\n    topK: options.topK || 5,\r\n    filter: options.filter,           // Pre-filter to reduce search space\r\n    includeMetadata: options.includeMetadata || false,\r\n    namespace: options.namespace,     // Use namespaces for isolation\r\n    \r\n    // Performance optimizations\r\n    approximateSearch: true,          // Trade accuracy for speed\r\n    searchTimeout: 2000,             // 2 second timeout\r\n    maxRetries: 2\r\n  });\r\n}\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Vector Store Benchmarks"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Compare vector store performance\r\nrag-pipeline benchmark retriever \\\r\n  --stores pinecone,chroma,weaviate \\\r\n  --queries 500 \\\r\n  --concurrent 10 \\\r\n  --metrics latency,throughput,accuracy\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-llm-generation-optimization",children:(0,i.jsx)(n.strong,{children:"3. LLM Generation Optimization"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Selection Strategy"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Choose optimal model for use case\r\nconst modelConfigs = {\r\n  'fast-responses': {\r\n    name: 'openai-gpt-3.5-turbo',\r\n    config: {\r\n      maxTokens: 500,\r\n      temperature: 0.3,\r\n      topP: 0.9,\r\n      frequencyPenalty: 0.1\r\n    }\r\n  },\r\n  'high-quality': {\r\n    name: 'openai-gpt-4',\r\n    config: {\r\n      maxTokens: 1500,\r\n      temperature: 0.7,\r\n      topP: 0.95\r\n    }\r\n  },\r\n  'cost-optimized': {\r\n    name: 'openai-gpt-3.5-turbo',\r\n    config: {\r\n      maxTokens: 300,\r\n      temperature: 0.1,\r\n      stop: ['\\n\\n', '###']\r\n    }\r\n  }\r\n};\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Streaming Optimization"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Implement streaming for better perceived performance\r\nasync function* optimizedGenerate(prompt, options = {}) {\r\n  const stream = await this.llm.generateStream(prompt, {\r\n    ...options,\r\n    bufferSize: 10,           // Buffer tokens for smoother streaming\r\n    flushInterval: 50,        // Flush every 50ms\r\n    earlyStop: true          // Stop on complete sentences\r\n  });\r\n\r\n  let buffer = '';\r\n  for await (const token of stream) {\r\n    buffer += token;\r\n    \r\n    // Flush on sentence boundaries for better UX\r\n    if (buffer.match(/[.!?]\\s/)) {\r\n      yield buffer;\r\n      buffer = '';\r\n    }\r\n  }\r\n  \r\n  if (buffer) yield buffer;\r\n}\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Token Usage Optimization"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Optimize prompts for token efficiency\r\nclass TokenOptimizer {\r\n  constructor(tokenizer) {\r\n    this.tokenizer = tokenizer;\r\n    this.maxContextTokens = 4096;\r\n    this.maxResponseTokens = 1000;\r\n  }\r\n\r\n  optimizePrompt(context, query) {\r\n    const basePrompt = `Context: {context}\\n\\nQuestion: ${query}\\n\\nAnswer:`;\r\n    const baseTokens = this.tokenizer.encode(basePrompt.replace('{context}', '')).length;\r\n    const availableTokens = this.maxContextTokens - baseTokens - this.maxResponseTokens;\r\n    \r\n    // Truncate context to fit token limit\r\n    const optimizedContext = this.truncateContext(context, availableTokens);\r\n    \r\n    return basePrompt.replace('{context}', optimizedContext);\r\n  }\r\n\r\n  truncateContext(context, maxTokens) {\r\n    const sentences = context.split(/[.!?]+/);\r\n    let truncated = '';\r\n    let tokenCount = 0;\r\n    \r\n    for (const sentence of sentences) {\r\n      const sentenceTokens = this.tokenizer.encode(sentence).length;\r\n      if (tokenCount + sentenceTokens &gt; maxTokens) break;\r\n      \r\n      truncated += sentence + '.';\r\n      tokenCount += sentenceTokens;\r\n    }\r\n    \r\n    return truncated;\r\n  }\r\n}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-pipeline-level-optimization",children:["\ud83d\udd04 ",(0,i.jsx)(n.strong,{children:"Pipeline-Level Optimization"})]}),"\n",(0,i.jsx)(n.h3,{id:"streaming-architecture",children:(0,i.jsx)(n.strong,{children:"Streaming Architecture"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Implement full pipeline streaming\r\nclass StreamingPipeline {\r\n  constructor(config) {\r\n    this.config = config;\r\n    this.bufferSize = config.bufferSize || 1000;\r\n    this.concurrency = config.concurrency || 3;\r\n  }\r\n\r\n  async* processStream(documents) {\r\n    const documentStream = this.createDocumentStream(documents);\r\n    const chunkStream = this.createChunkStream(documentStream);\r\n    const embeddingStream = this.createEmbeddingStream(chunkStream);\r\n    \r\n    for await (const batch of embeddingStream) {\r\n      // Process embeddings in parallel\r\n      const results = await Promise.allSettled(\r\n        batch.map(item =&gt; this.processEmbedding(item))\r\n      );\r\n      \r\n      yield results.filter(r =&gt; r.status === 'fulfilled').map(r =&gt; r.value);\r\n    }\r\n  }\r\n\r\n  async* createChunkStream(documentStream) {\r\n    let buffer = [];\r\n    \r\n    for await (const document of documentStream) {\r\n      const chunks = await this.chunkDocument(document);\r\n      buffer.push(...chunks);\r\n      \r\n      if (buffer.length &gt;= this.bufferSize) {\r\n        yield buffer.splice(0, this.bufferSize);\r\n      }\r\n    }\r\n    \r\n    if (buffer.length &gt; 0) yield buffer;\r\n  }\r\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"memory-management",children:(0,i.jsx)(n.strong,{children:"Memory Management"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Implement memory-efficient processing\r\nclass MemoryOptimizedPipeline {\r\n  constructor(options = {}) {\r\n    this.maxMemoryUsage = options.maxMemory || '2GB';\r\n    this.gcThreshold = options.gcThreshold || 0.8;\r\n    this.batchSize = options.batchSize || 100;\r\n  }\r\n\r\n  async processLargeDataset(documents) {\r\n    const batches = this.createBatches(documents, this.batchSize);\r\n    \r\n    for (const batch of batches) {\r\n      // Monitor memory usage\r\n      const memoryUsage = process.memoryUsage();\r\n      const memoryPercent = memoryUsage.heapUsed / memoryUsage.heapTotal;\r\n      \r\n      if (memoryPercent &gt; this.gcThreshold) {\r\n        // Force garbage collection\r\n        if (global.gc) global.gc();\r\n        \r\n        // Reduce batch size if memory pressure continues\r\n        if (memoryPercent &gt; 0.9) {\r\n          this.batchSize = Math.max(10, Math.floor(this.batchSize * 0.8));\r\n        }\r\n      }\r\n      \r\n      await this.processBatch(batch);\r\n      \r\n      // Clear batch references\r\n      batch.length = 0;\r\n    }\r\n  }\r\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"caching-strategy",children:(0,i.jsx)(n.strong,{children:"Caching Strategy"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Multi-level caching implementation\r\nclass CacheManager {\r\n  constructor(options = {}) {\r\n    // L1: In-memory cache (fastest)\r\n    this.l1Cache = new LRUCache({ \r\n      max: options.l1Size || 1000,\r\n      ttl: options.l1TTL || 300000 // 5 minutes\r\n    });\r\n    \r\n    // L2: Redis cache (shared across instances)\r\n    this.l2Cache = new RedisCache({\r\n      host: options.redisHost,\r\n      ttl: options.l2TTL || 3600000 // 1 hour\r\n    });\r\n    \r\n    // L3: Disk cache (persistent)\r\n    this.l3Cache = new DiskCache({\r\n      directory: options.cacheDir || './cache',\r\n      maxSize: options.l3Size || '1GB'\r\n    });\r\n  }\r\n\r\n  async get(key) {\r\n    // Try L1 first\r\n    let value = this.l1Cache.get(key);\r\n    if (value) return value;\r\n    \r\n    // Try L2\r\n    value = await this.l2Cache.get(key);\r\n    if (value) {\r\n      this.l1Cache.set(key, value);\r\n      return value;\r\n    }\r\n    \r\n    // Try L3\r\n    value = await this.l3Cache.get(key);\r\n    if (value) {\r\n      this.l1Cache.set(key, value);\r\n      this.l2Cache.set(key, value);\r\n      return value;\r\n    }\r\n    \r\n    return null;\r\n  }\r\n\r\n  async set(key, value) {\r\n    this.l1Cache.set(key, value);\r\n    await this.l2Cache.set(key, value);\r\n    await this.l3Cache.set(key, value);\r\n  }\r\n}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-monitoring--profiling",children:["\ud83d\udcc8 ",(0,i.jsx)(n.strong,{children:"Monitoring & Profiling"})]}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics-collection",children:(0,i.jsx)(n.strong,{children:"Performance Metrics Collection"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Built-in performance monitoring\r\nimport { createPerformanceMonitor } from '@DevilsDev/rag-pipeline-utils';\r\n\r\nconst monitor = createPerformanceMonitor({\r\n  metrics: ['throughput', 'latency', 'memory', 'tokens', 'errors'],\r\n  interval: 1000,           // Collect every second\r\n  retention: 3600,          // Keep 1 hour of data\r\n  alerts: {\r\n    highLatency: { threshold: 5000, action: 'log' },\r\n    memoryUsage: { threshold: 0.9, action: 'gc' },\r\n    errorRate: { threshold: 0.05, action: 'alert' }\r\n  }\r\n});\r\n\r\nconst pipeline = createRagPipeline({\r\n  monitor,\r\n  // ... other config\r\n});\r\n\r\n// Access performance data\r\nconst metrics = monitor.getMetrics();\r\nconsole.log('Average latency:', metrics.latency.average);\r\nconsole.log('Throughput:', metrics.throughput.current);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"profiling-tools",children:(0,i.jsx)(n.strong,{children:"Profiling Tools"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Profile memory usage\r\nnode --inspect --max-old-space-size=4096 rag-pipeline ingest ./large-dataset\r\n\r\n# Profile CPU usage\r\nrag-pipeline benchmark --profile-cpu --output cpu-profile.json\r\n\r\n# Generate performance report\r\nrag-pipeline analyze-performance \\\r\n  --input performance-logs.json \\\r\n  --output performance-report.html \\\r\n  --include-recommendations\n"})}),"\n",(0,i.jsx)(n.h3,{id:"custom-metrics",children:(0,i.jsx)(n.strong,{children:"Custom Metrics"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Define custom performance metrics\r\nclass CustomMetrics {\r\n  constructor() {\r\n    this.metrics = new Map();\r\n    this.startTimes = new Map();\r\n  }\r\n\r\n  startTimer(operation) {\r\n    this.startTimes.set(operation, Date.now());\r\n  }\r\n\r\n  endTimer(operation) {\r\n    const startTime = this.startTimes.get(operation);\r\n    if (startTime) {\r\n      const duration = Date.now() - startTime;\r\n      this.recordMetric(operation, duration);\r\n      this.startTimes.delete(operation);\r\n    }\r\n  }\r\n\r\n  recordMetric(name, value) {\r\n    if (!this.metrics.has(name)) {\r\n      this.metrics.set(name, []);\r\n    }\r\n    this.metrics.get(name).push({\r\n      value,\r\n      timestamp: Date.now()\r\n    });\r\n  }\r\n\r\n  getStats(name) {\r\n    const values = this.metrics.get(name) || [];\r\n    if (values.length === 0) return null;\r\n    \r\n    const nums = values.map(v =&gt; v.value);\r\n    return {\r\n      count: nums.length,\r\n      average: nums.reduce((a, b) =&gt; a + b, 0) / nums.length,\r\n      min: Math.min(...nums),\r\n      max: Math.max(...nums),\r\n      p95: this.percentile(nums, 0.95),\r\n      p99: this.percentile(nums, 0.99)\r\n    };\r\n  }\r\n}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-production-optimization",children:["\u26a1 ",(0,i.jsx)(n.strong,{children:"Production Optimization"})]}),"\n",(0,i.jsx)(n.h3,{id:"deployment-configuration",children:(0,i.jsx)(n.strong,{children:"Deployment Configuration"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Docker optimization\r\nFROM node:18-alpine\r\nWORKDIR /app\r\n\r\n# Optimize Node.js for production\r\nENV NODE_ENV=production\r\nENV NODE_OPTIONS="--max-old-space-size=4096 --optimize-for-size"\r\n\r\n# Install dependencies\r\nCOPY package*.json ./\r\nRUN npm ci --only=production --no-audit\r\n\r\n# Copy application\r\nCOPY . .\r\n\r\n# Optimize startup\r\nCMD ["node", "--experimental-worker", "index.js"]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"load-balancing",children:(0,i.jsx)(n.strong,{children:"Load Balancing"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Horizontal scaling with load balancing\r\nclass LoadBalancer {\r\n  constructor(instances) {\r\n    this.instances = instances;\r\n    this.currentIndex = 0;\r\n    this.healthChecks = new Map();\r\n  }\r\n\r\n  async getHealthyInstance() {\r\n    const startIndex = this.currentIndex;\r\n    \r\n    do {\r\n      const instance = this.instances[this.currentIndex];\r\n      this.currentIndex = (this.currentIndex + 1) % this.instances.length;\r\n      \r\n      if (await this.isHealthy(instance)) {\r\n        return instance;\r\n      }\r\n    } while (this.currentIndex !== startIndex);\r\n    \r\n    throw new Error('No healthy instances available');\r\n  }\r\n\r\n  async isHealthy(instance) {\r\n    try {\r\n      const response = await instance.healthCheck();\r\n      return response.status === 'healthy';\r\n    } catch (error) {\r\n      return false;\r\n    }\r\n  }\r\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"auto-scaling-configuration",children:(0,i.jsx)(n.strong,{children:"Auto-scaling Configuration"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Auto-scaling based on load\r\nclass AutoScaler {\r\n  constructor(options = {}) {\r\n    this.minInstances = options.min || 1;\r\n    this.maxInstances = options.max || 10;\r\n    this.targetCPU = options.targetCPU || 0.7;\r\n    this.scaleUpThreshold = options.scaleUpThreshold || 0.8;\r\n    this.scaleDownThreshold = options.scaleDownThreshold || 0.3;\r\n    this.instances = [];\r\n  }\r\n\r\n  async monitor() {\r\n    const metrics = await this.collectMetrics();\r\n    const avgCPU = metrics.cpu.average;\r\n    const avgMemory = metrics.memory.average;\r\n    \r\n    if (avgCPU &gt; this.scaleUpThreshold && this.instances.length &lt; this.maxInstances) {\r\n      await this.scaleUp();\r\n    } else if (avgCPU &lt; this.scaleDownThreshold && this.instances.length &gt; this.minInstances) {\r\n      await this.scaleDown();\r\n    }\r\n  }\r\n\r\n  async scaleUp() {\r\n    const newInstance = await this.createInstance();\r\n    this.instances.push(newInstance);\r\n    console.log(`Scaled up to ${this.instances.length} instances`);\r\n  }\r\n\r\n  async scaleDown() {\r\n    const instance = this.instances.pop();\r\n    await this.terminateInstance(instance);\r\n    console.log(`Scaled down to ${this.instances.length} instances`);\r\n  }\r\n}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-performance-best-practices",children:["\ud83c\udfaf ",(0,i.jsx)(n.strong,{children:"Performance Best Practices"})]}),"\n",(0,i.jsx)(n.h3,{id:"1-configuration-optimization",children:(0,i.jsx)(n.strong,{children:"1. Configuration Optimization"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\r\n  "performance": {\r\n    "embedder": {\r\n      "batchSize": 100,\r\n      "maxConcurrency": 5,\r\n      "cacheEnabled": true,\r\n      "timeout": 30000\r\n    },\r\n    "retriever": {\r\n      "topK": 5,\r\n      "approximateSearch": true,\r\n      "connectionPoolSize": 10\r\n    },\r\n    "llm": {\r\n      "maxTokens": 1000,\r\n      "temperature": 0.3,\r\n      "streaming": true\r\n    },\r\n    "pipeline": {\r\n      "parallelProcessing": true,\r\n      "memoryLimit": "2GB",\r\n      "gcThreshold": 0.8\r\n    }\r\n  }\r\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-resource-management",children:(0,i.jsx)(n.strong,{children:"2. Resource Management"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory"}),": Use streaming for large datasets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CPU"}),": Leverage parallel processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Network"}),": Implement connection pooling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Storage"}),": Use appropriate caching strategies"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-monitoring-checklist",children:(0,i.jsx)(n.strong,{children:"3. Monitoring Checklist"})}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Track response latency (target: less than 2s)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Monitor throughput (documents/second)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Watch memory usage (keep under 80%)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Monitor error rates (keep under 1%)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Track token usage costs"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up alerting for anomalies"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-performance-benchmarks",children:["\ud83d\udcca ",(0,i.jsx)(n.strong,{children:"Performance Benchmarks"})]}),"\n",(0,i.jsx)(n.h3,{id:"reference-performance",children:(0,i.jsx)(n.strong,{children:"Reference Performance"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Component"}),(0,i.jsx)(n.th,{children:"Throughput"}),(0,i.jsx)(n.th,{children:"Latency (P95)"}),(0,i.jsx)(n.th,{children:"Memory Usage"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"PDF Loader"}),(0,i.jsx)(n.td,{children:"50 docs/sec"}),(0,i.jsx)(n.td,{children:"200ms"}),(0,i.jsx)(n.td,{children:"100MB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"OpenAI Embedder"}),(0,i.jsx)(n.td,{children:"1000 texts/sec"}),(0,i.jsx)(n.td,{children:"500ms"}),(0,i.jsx)(n.td,{children:"50MB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Pinecone Retriever"}),(0,i.jsx)(n.td,{children:"500 queries/sec"}),(0,i.jsx)(n.td,{children:"100ms"}),(0,i.jsx)(n.td,{children:"25MB"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"GPT-4 Generation"}),(0,i.jsx)(n.td,{children:"10 queries/sec"}),(0,i.jsx)(n.td,{children:"3000ms"}),(0,i.jsx)(n.td,{children:"75MB"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"optimization-targets",children:(0,i.jsx)(n.strong,{children:"Optimization Targets"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embedding"}),": greater than 500 texts/second"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Retrieval"}),": under 200ms P95 latency"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generation"}),": under 5s P95 latency"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory"}),": under 1GB for 10k documents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Rate"}),": under 0.5%"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["This performance guide provides comprehensive optimization strategies for @DevilsDev/rag-pipeline-utils. For troubleshooting performance issues, see the ",(0,i.jsx)(n.a,{href:"/rag-pipeline-utils/docs/Troubleshooting",children:"Troubleshooting Guide"}),"."]})})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var t=r(6540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);