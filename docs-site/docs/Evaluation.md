# Evaluation

This module allows you to evaluate the quality of generated LLM responses in a RAG pipeline.

---

## ðŸŽ¯ Objective

Quantify how well the pipeline performs on question-answer pairs using:
- **BLEU**: Precision on matching tokens
- **ROUGE**: Recall + overlap on longest subsequences
- **LLM Feedback** *(planned)*: Score generation based on LLM ratings

---

## ðŸ“„ Dataset Format

Each entry in your JSON dataset should include:

```json
{
  "prompt": "What is a vector index?",
  "groundTruth": "A data structure for fast similarity search in vector spaces."
}
```

Saved as: `sample-eval-dataset.json`

---

## ðŸš€ CLI Evaluation

```bash
rag-pipeline evaluate ./fixtures/sample-eval-dataset.json
```

Outputs:
- âœ… Pass/fail per item
- ðŸ“Š BLEU/ROUGE scores
- ðŸ“ˆ Aggregated metrics

---

## ðŸ“Š Dashboard Visualization

Use the React dashboard under `/public`:

```bash
node server.js
```

Visit: [http://localhost:3000](http://localhost:3000)

Displays:
- Prompt vs Answer
- Success criteria
- Metric charts (BLEU, ROUGE)

---

Next â†’ [Plugins](./Plugins.md)
